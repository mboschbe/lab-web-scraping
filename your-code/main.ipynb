{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeej it works\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import requests\n",
    "link = 'https://github.com/trending/developers'\n",
    "page = requests.get(link)\n",
    "\n",
    "# error handling\n",
    "if(page.status_code==200):\n",
    "    print('jeej it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "    print('status_code: ', page.status_code)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3cdf61a161f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "list(soup.children)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools or clicking in 'Inspect' on any browser. Here is an example:\n",
    "\n",
    "![title](example_1.png)\n",
    "\n",
    "2. Use BeautifulSoup `find_all()` to extract all the html elements that contain the developer names. Hint: pass in the `attrs` parameter to specify the class.\n",
    "\n",
    "3. Loop through the elements found and get the text for each of them.\n",
    "\n",
    "4. While you are at it, use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names. Hint: you may also use `.get_text()` instead of `.text` and pass in the desired parameters to do some string manipulation (check the documentation).\n",
    "\n",
    "5. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "name = soup.find_all('p',{'class':\"f4 text-normal mb-1\"})\n",
    "nickname = soup.find_all('h1',{'class':\"h3 lh-condensed\"})\n",
    "\n",
    "\n",
    "name2 = [soup.find_all(class_=\"h3 lh-condensed\")[i].get_text().replace('\\n\\n            ','').replace('\\n ','') for i in range(0,24)]\n",
    "\n",
    "nickname2 = [soup.find_all(class_=\"f4 text-normal mb-1\")[i].get_text().replace('\\n\\n            ','').replace('\\n ','') for i in range(0,24)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ariel Mashraki(a8m)',\n",
       " 'Javier Suárez(jsuarezruiz)',\n",
       " 'David Rodríguez(deivid-rodriguez)',\n",
       " 'Luís Cobucci(lcobucci)',\n",
       " 'Mariusz Nowak(medikoo)',\n",
       " \"Debbie O'Brien(debs-obrien)\",\n",
       " 'Sean McArthur(seanmonstar)',\n",
       " 'Remi Rousselet(rrousselGit)',\n",
       " 'Andrew Kane(ankane)',\n",
       " 'francisco souza(fsouza)',\n",
       " 'Miek Gieben(miekg)',\n",
       " 'Bartlomiej Plotka(bwplotka)',\n",
       " 'Geoff Boeing(gboeing)',\n",
       " 'Carlos Cuesta(carloscuesta)',\n",
       " 'Phil Pluckthun(kitten)',\n",
       " 'Tobias Koppers(sokra)',\n",
       " 'Nisar Hassan Naqvi(nisarhassan12)',\n",
       " 'Jason Quense(jquense)',\n",
       " 'XhmikosR(aeneasr)',\n",
       " 'hackerman(mpariente)',\n",
       " 'Pariente Manuel(wojtekmaj)',\n",
       " 'Wojciech Maj(stsrki)',\n",
       " 'Mladen Macanović(kzu)',\n",
       " 'Daniel Cazzulino(balloob)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = [soup.find_all(class_=\"h3 lh-condensed\")[i].get_text().replace('\\n\\n            ','').replace('\\n ','')+ '(' + soup.find_all('p',{'class':'f4 text-normal mb-1'})[i].get_text().replace(\"\\n\\n              \", \"\" ).replace(\"\\n \", \"\") + ')' for i in range(0,24)]\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeej it works\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import requests\n",
    "link = 'https://github.com/trending/python?since=daily'\n",
    "page = requests.get(link)\n",
    "\n",
    "# error handling\n",
    "if(page.status_code==200):\n",
    "    print('jeej it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "    print('status_code: ', page.status_code)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1ccfe1427d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n        TheAlgorithms /\\n\\n      Python\\n ',\n",
       " '\\n\\n\\n\\n        vinta /\\n\\n      awesome-python\\n ',\n",
       " '\\n\\n\\n\\n        trekhleb /\\n\\n      learn-python\\n ',\n",
       " '\\n\\n\\n\\n        jerry-git /\\n\\n      learn-python3\\n ',\n",
       " '\\n\\n\\n\\n        gradslam /\\n\\n      gradslam\\n ',\n",
       " '\\n\\n\\n\\n        ansible /\\n\\n      ansible\\n ',\n",
       " '\\n\\n\\n\\n        3b1b /\\n\\n      manim\\n ',\n",
       " '\\n\\n\\n\\n        donnemartin /\\n\\n      system-design-primer\\n ',\n",
       " '\\n\\n\\n\\n        y1ndan /\\n\\n      genshin-impact-helper\\n ',\n",
       " '\\n\\n\\n\\n        tensorflow /\\n\\n      models\\n ',\n",
       " '\\n\\n\\n\\n        Hari-Nagarajan /\\n\\n      nvidia-bot\\n ',\n",
       " '\\n\\n\\n\\n        edx /\\n\\n      edx-platform\\n ',\n",
       " '\\n\\n\\n\\n        apache /\\n\\n      airflow\\n ',\n",
       " '\\n\\n\\n\\n        streamlit /\\n\\n      streamlit\\n ',\n",
       " '\\n\\n\\n\\n        lorenzodifuccia /\\n\\n      safaribooks\\n ',\n",
       " '\\n\\n\\n\\n        microsoft /\\n\\n      cascadia-code\\n ',\n",
       " '\\n\\n\\n\\n        jupyterhub /\\n\\n      jupyterhub\\n ',\n",
       " '\\n\\n\\n\\n        junyanz /\\n\\n      pytorch-CycleGAN-and-pix2pix\\n ',\n",
       " '\\n\\n\\n\\n        jackfrued /\\n\\n      Python-100-Days\\n ',\n",
       " '\\n\\n\\n\\n        returntocorp /\\n\\n      semgrep\\n ',\n",
       " '\\n\\n\\n\\n        RasaHQ /\\n\\n      rasa\\n ',\n",
       " '\\n\\n\\n\\n        openai /\\n\\n      gym\\n ',\n",
       " '\\n\\n\\n\\n        encode /\\n\\n      django-rest-framework\\n ',\n",
       " '\\n\\n\\n\\n        QUANTAXIS /\\n\\n      QUANTAXIS\\n ',\n",
       " '\\n\\n\\n\\n        facebookresearch /\\n\\n      pytorch3d\\n ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= [soup.find_all(class_=\"h3 lh-condensed\")[i].get_text() for i in range(0,25)]\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TheAlgorithms - Python ',\n",
       " 'vinta - awesome-python ',\n",
       " 'trekhleb - learn-python ',\n",
       " 'jerry-git - learn-python3 ',\n",
       " 'gradslam - gradslam ',\n",
       " 'ansible - ansible ',\n",
       " '3b1b - manim ',\n",
       " 'donnemartin - system-design-primer ',\n",
       " 'y1ndan - genshin-impact-helper ',\n",
       " 'tensorflow - models ',\n",
       " 'Hari-Nagarajan - nvidia-bot ',\n",
       " 'edx - edx-platform ',\n",
       " 'apache - airflow ',\n",
       " 'streamlit - streamlit ',\n",
       " 'lorenzodifuccia - safaribooks ',\n",
       " 'microsoft - cascadia-code ',\n",
       " 'jupyterhub - jupyterhub ',\n",
       " 'junyanz - pytorch-CycleGAN-and-pix2pix ',\n",
       " 'jackfrued - Python-100-Days ',\n",
       " 'returntocorp - semgrep ',\n",
       " 'RasaHQ - rasa ',\n",
       " 'openai - gym ',\n",
       " 'encode - django-rest-framework ',\n",
       " 'QUANTAXIS - QUANTAXIS ',\n",
       " 'facebookresearch - pytorch3d ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = soup.find_all('h1',{'class':\"h3 lh-condensed\"})[0]\n",
    "\n",
    "trending_repo = [soup.find_all(class_=\"h3 lh-condensed\")[i].get_text().replace('\\n\\n\\n\\n        ','').replace('/\\n\\n      ','- ').replace(\"\\n \",\" \") for i in range(0,25)]\n",
    "\n",
    "trending_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display all the image links from Walt Disney wikipedia page.\n",
    "Hint: use `.get()` to access information inside tags. Check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg\n",
      "\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Walt_Disney')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "images = bs.find_all('img', {'src':re.compile('.jpg')})\n",
    "for image in images:\n",
    "    print(image['src']+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeej it works\n"
     ]
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "\n",
    "import requests\n",
    "link = \"https://www.wikipedia.org/\"\n",
    "page = requests.get(link)\n",
    "\n",
    "# error handling\n",
    "if(page.status_code==200):\n",
    "    print('jeej it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "    print('status_code: ', page.status_code)\n",
    "    \n",
    "    \n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "list(soup.children)\n",
    "\n",
    "test = soup.find_all(\"strong\")\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " 'Español',\n",
       " '日本語',\n",
       " 'Deutsch',\n",
       " 'Русский',\n",
       " 'Français',\n",
       " 'Italiano',\n",
       " '中文',\n",
       " 'Português',\n",
       " 'Polski']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = [soup.find_all(\"strong\")[i].get_text() for i in range(1,11)]\n",
    "\n",
    "languages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6183000+',\n",
       " '1637000+',\n",
       " '1235000+',\n",
       " '2495000+',\n",
       " '1672000+',\n",
       " '2262000+',\n",
       " '1645000+',\n",
       " '1155000+',\n",
       " '1045000+',\n",
       " '1435000+']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = [soup.find_all(\"bdi\")[i].get_text().replace(\"\\xa0\",\"\") for i in range(0,10)]\n",
    "\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('English', '6183000+'), ('Español', '1637000+'), ('日本語', '1235000+'), ('Deutsch', '2495000+'), ('Русский', '1672000+'), ('Français', '2262000+'), ('Italiano', '1645000+'), ('中文', '1155000+'), ('Português', '1045000+'), ('Polski', '1435000+')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(languages,articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Display the top 10 languages by number of native speakers stored in a pandas dataframe.\n",
    "Hint: After finding the correct table you want to analyse, you can use a nested **for** loop to find the elements row by row (check out the 'td' and 'tr' tags). <br>An easier way to do it is using pd.read_html(), check out documentation [here](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>% of World pop.(March 2019)[8]</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918.0</td>\n",
       "      <td>11.922</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480.0</td>\n",
       "      <td>5.994</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379.0</td>\n",
       "      <td>4.922</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (Sanskritised Hindustani)[9]</td>\n",
       "      <td>341.0</td>\n",
       "      <td>4.429</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228.0</td>\n",
       "      <td>2.961</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221.0</td>\n",
       "      <td>2.870</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.662</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[10]</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                            Language  Speakers(millions)  \\\n",
       "0     1                    Mandarin Chinese               918.0   \n",
       "1     2                             Spanish               480.0   \n",
       "2     3                             English               379.0   \n",
       "3     4  Hindi (Sanskritised Hindustani)[9]               341.0   \n",
       "4     5                             Bengali               228.0   \n",
       "5     6                          Portuguese               221.0   \n",
       "6     7                             Russian               154.0   \n",
       "7     8                            Japanese               128.0   \n",
       "8     9                 Western Punjabi[10]                92.7   \n",
       "9    10                             Marathi                83.1   \n",
       "\n",
       "   % of World pop.(March 2019)[8] Language family        Branch  \n",
       "0                          11.922    Sino-Tibetan       Sinitic  \n",
       "1                           5.994   Indo-European       Romance  \n",
       "2                           4.922   Indo-European      Germanic  \n",
       "3                           4.429   Indo-European    Indo-Aryan  \n",
       "4                           2.961   Indo-European    Indo-Aryan  \n",
       "5                           2.870   Indo-European       Romance  \n",
       "6                           2.000   Indo-European  Balto-Slavic  \n",
       "7                           1.662         Japonic      Japanese  \n",
       "8                           1.204   Indo-European    Indo-Aryan  \n",
       "9                           1.079   Indo-European    Indo-Aryan  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "data = pd.read_html(url)\n",
    "data[0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe.\n",
    "Hint: If you hover over the title of the movie, you should see the director's name. Can you find where it's stored in the html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9f5d4af7eb1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "data = pd.read_html(url)\n",
    "data\n",
    "\n",
    "import requests\n",
    "link = 'https://www.imdb.com/chart/top'\n",
    "page = requests.get(link)\n",
    "\n",
    "# error handling\n",
    "if(page.status_code==200):\n",
    "    print('jeej it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "    print('status_code: ', page.status_code)\n",
    "    \n",
    "    \n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "\n",
    "list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/title/tt0068646/\" title=\"Francis Ford Coppola (dir.), Marlon Brando, Al Pacino\">El padrino</a>]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Francis Ford Coppola (dir.)', ' Marlon Brando', ' Al Pacino']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Francis Ford Coppola (dir.)'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_directors = list(soup.find_all(class_='titleColumn')[1].find_all('a'))\n",
    "for director in test_directors:\n",
    "    print(director.get('title').split(','))\n",
    "\n",
    "test_directors[0].get('title').split(',')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frank Darabont',\n",
       " 'Francis Ford Coppola',\n",
       " 'Francis Ford Coppola',\n",
       " 'Christopher Nolan',\n",
       " 'Sidney Lumet',\n",
       " 'Steven Spielberg',\n",
       " 'Peter Jackson',\n",
       " 'Quentin Tarantino',\n",
       " 'Sergio Leone',\n",
       " 'Peter Jackson',\n",
       " 'David Fincher',\n",
       " 'Robert Zemeckis',\n",
       " 'Christopher Nolan',\n",
       " 'Peter Jackson',\n",
       " 'Irvin Kershner',\n",
       " 'Lana Wachowski',\n",
       " 'Martin Scorsese',\n",
       " 'Milos Forman',\n",
       " 'Akira Kurosawa',\n",
       " 'David Fincher',\n",
       " 'Roberto Benigni',\n",
       " 'Fernando Meirelles',\n",
       " 'Jonathan Demme',\n",
       " 'Frank Capra',\n",
       " 'George Lucas',\n",
       " 'Steven Spielberg',\n",
       " 'Hayao Miyazaki',\n",
       " 'Frank Darabont',\n",
       " 'Christopher Nolan',\n",
       " 'Bong Joon Ho',\n",
       " 'Luc Besson',\n",
       " 'Bryan Singer',\n",
       " 'Masaki Kobayashi',\n",
       " 'Roger Allers',\n",
       " 'Robert Zemeckis',\n",
       " 'Roman Polanski',\n",
       " 'James Cameron',\n",
       " 'Tony Kaye',\n",
       " 'Charles Chaplin',\n",
       " 'Alfred Hitchcock',\n",
       " 'Ridley Scott',\n",
       " 'Charles Chaplin',\n",
       " 'Martin Scorsese',\n",
       " 'Olivier Nakache',\n",
       " 'Damien Chazelle',\n",
       " 'Christopher Nolan',\n",
       " 'Isao Takahata',\n",
       " 'Thomas Kail',\n",
       " 'Sergio Leone',\n",
       " 'Michael Curtiz',\n",
       " 'Giuseppe Tornatore',\n",
       " 'Alfred Hitchcock',\n",
       " 'Ridley Scott',\n",
       " 'Francis Ford Coppola',\n",
       " 'Christopher Nolan',\n",
       " 'Charles Chaplin',\n",
       " 'Steven Spielberg',\n",
       " 'Quentin Tarantino',\n",
       " 'Florian Henckel von Donnersmarck',\n",
       " 'Todd Phillips',\n",
       " 'Stanley Kubrick',\n",
       " 'Andrew Stanton',\n",
       " 'Stanley Kubrick',\n",
       " 'Anthony Russo',\n",
       " 'Billy Wilder',\n",
       " 'Billy Wilder',\n",
       " 'Chan-wook Park',\n",
       " 'Bob Persichetti',\n",
       " 'Hayao Miyazaki',\n",
       " 'Stanley Kubrick',\n",
       " 'Christopher Nolan',\n",
       " 'Sergio Leone',\n",
       " 'James Cameron',\n",
       " 'Makoto Shinkai',\n",
       " 'Anthony Russo',\n",
       " 'Lee Unkrich',\n",
       " 'Sam Mendes',\n",
       " 'Mel Gibson',\n",
       " 'Wolfgang Petersen',\n",
       " 'Rajkumar Hirani',\n",
       " 'John Lasseter',\n",
       " 'Akira Kurosawa',\n",
       " 'Nadine Labaki',\n",
       " 'Milos Forman',\n",
       " 'Quentin Tarantino',\n",
       " 'Richard Marquand',\n",
       " 'Aamir Khan',\n",
       " 'Gus Van Sant',\n",
       " 'Quentin Tarantino',\n",
       " 'Stanley Kubrick',\n",
       " 'Darren Aronofsky',\n",
       " 'Alfred Hitchcock',\n",
       " 'Fritz Lang',\n",
       " 'Michel Gondry',\n",
       " 'Thomas Vinterberg',\n",
       " 'Nitesh Tiwari',\n",
       " 'Orson Welles',\n",
       " 'Sam Mendes',\n",
       " 'Stanley Kubrick',\n",
       " 'Vittorio De Sica',\n",
       " 'Stanley Donen',\n",
       " 'Charles Chaplin',\n",
       " 'Stanley Kubrick',\n",
       " 'Alfred Hitchcock',\n",
       " 'Guy Ritchie',\n",
       " 'Brian De Palma',\n",
       " 'Martin Scorsese',\n",
       " 'Akira Kurosawa',\n",
       " 'Jean-Pierre Jeunet',\n",
       " 'David Lean',\n",
       " 'Lee Unkrich',\n",
       " 'George Roy Hill',\n",
       " 'Fritz Lang',\n",
       " 'Asghar Farhadi',\n",
       " 'Denis Villeneuve',\n",
       " 'Elem Klimov',\n",
       " 'Sergio Leone',\n",
       " 'Billy Wilder',\n",
       " 'Billy Wilder',\n",
       " 'Robert Mulligan',\n",
       " 'Pete Docter',\n",
       " 'Steven Spielberg',\n",
       " 'Michael Mann',\n",
       " 'Curtis Hanson',\n",
       " 'John McTiernan',\n",
       " 'Terry Gilliam',\n",
       " 'Akira Kurosawa',\n",
       " 'Akira Kurosawa',\n",
       " 'Christopher Nolan',\n",
       " 'Peter Farrelly',\n",
       " 'Oliver Hirschbiegel',\n",
       " 'Majid Majidi',\n",
       " 'Clint Eastwood',\n",
       " 'Akira Kurosawa',\n",
       " 'Billy Wilder',\n",
       " 'Hayao Miyazaki',\n",
       " 'Ron Howard',\n",
       " 'Joseph L. Mankiewicz',\n",
       " 'Martin Scorsese',\n",
       " 'John Sturges',\n",
       " 'Martin Scorsese',\n",
       " 'Guillermo del Toro',\n",
       " 'Juan José Campanella',\n",
       " 'Guy Ritchie',\n",
       " 'Paul Thomas Anderson',\n",
       " 'Martin Scorsese',\n",
       " 'Hayao Miyazaki',\n",
       " 'Stanley Kramer',\n",
       " 'Hrishikesh Mukherjee',\n",
       " 'John Huston',\n",
       " 'Alfred Hitchcock',\n",
       " 'Martin McDonagh',\n",
       " 'Roman Polanski',\n",
       " 'Charles Chaplin',\n",
       " 'Martin Scorsese',\n",
       " 'Çagan Irmak',\n",
       " 'Ethan Coen',\n",
       " 'James McTeigue',\n",
       " 'Ingmar Bergman',\n",
       " 'Pete Docter',\n",
       " \"Gavin O'Connor\",\n",
       " 'David Lynch',\n",
       " 'John Carpenter',\n",
       " 'M. Night Shyamalan',\n",
       " 'Danny Boyle',\n",
       " 'Steven Spielberg',\n",
       " 'Victor Fleming',\n",
       " 'Peter Weir',\n",
       " 'Andrew Stanton',\n",
       " 'Ingmar Bergman',\n",
       " 'Ridley Scott',\n",
       " 'Andrei Tarkovsky',\n",
       " 'Quentin Tarantino',\n",
       " 'David Lean',\n",
       " 'Lenny Abrahamson',\n",
       " 'Bong Joon Ho',\n",
       " 'Joel Coen',\n",
       " 'Yasujirô Ozu',\n",
       " 'Clint Eastwood',\n",
       " 'Carol Reed',\n",
       " 'Elia Kazan',\n",
       " 'Damián Szifron',\n",
       " 'Michael Cimino',\n",
       " 'Sergio Pablos',\n",
       " 'Jim Sheridan',\n",
       " 'Adam Elliot',\n",
       " 'David Fincher',\n",
       " 'Wes Anderson',\n",
       " 'Mel Gibson',\n",
       " 'Richard Linklater',\n",
       " 'Sriram Raghavan',\n",
       " 'Steven Spielberg',\n",
       " 'Gayatri',\n",
       " 'Ingmar Bergman',\n",
       " 'Joel Coen',\n",
       " 'Denis Villeneuve',\n",
       " 'Ernst Lubitsch',\n",
       " 'Buster Keaton',\n",
       " 'Yavuz Turgul',\n",
       " 'Clyde Bruckman',\n",
       " 'Dean DeBlois',\n",
       " 'James Mangold',\n",
       " 'Steve McQueen',\n",
       " 'Frank Capra',\n",
       " 'Stanley Kubrick',\n",
       " 'George Miller',\n",
       " 'Clint Eastwood',\n",
       " 'Sidney Lumet',\n",
       " 'Rob Reiner',\n",
       " 'Peter Weir',\n",
       " 'Stuart Rosenberg',\n",
       " 'William Wyler',\n",
       " 'Lasse Hallström',\n",
       " 'David Yates',\n",
       " 'Oliver Stone',\n",
       " 'Sean Penn',\n",
       " 'James Mangold',\n",
       " 'Henri-Georges Clouzot',\n",
       " 'Terry Jones',\n",
       " 'Chan-wook Park',\n",
       " 'Ron Howard',\n",
       " 'François Truffaut',\n",
       " 'Carl Theodor Dreyer',\n",
       " 'Andrei Tarkovsky',\n",
       " 'Tom McCarthy',\n",
       " 'Terry George',\n",
       " 'Mathieu Kassovitz',\n",
       " 'Alejandro G. Iñárritu',\n",
       " 'Jules Dassin',\n",
       " 'John G. Avildsen',\n",
       " 'Hayao Miyazaki',\n",
       " 'Anurag Kashyap',\n",
       " 'Pete Docter',\n",
       " 'Alfred Hitchcock',\n",
       " 'Rakeysh Omprakash Mehra',\n",
       " 'Richard Linklater',\n",
       " 'Kar-Wai Wong',\n",
       " 'Céline Sciamma',\n",
       " 'Wim Wenders',\n",
       " 'Frank Capra',\n",
       " 'Nishikant Kamat',\n",
       " 'Oriol Paulo',\n",
       " 'Rob Reiner',\n",
       " 'Tate Taylor',\n",
       " 'Gillo Pontecorvo',\n",
       " 'Charles Chaplin',\n",
       " 'James Cameron',\n",
       " 'Zaza Urushadze',\n",
       " 'Naoko Yamada',\n",
       " 'Nuri Bilge Ceylan']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [list(soup.find_all(class_='titleColumn')[i].find_all('a'))[0].get('title').split(',')[0].replace(\" (dir.)\",\"\") for i in range(0,250)]\n",
    "\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>year</th>\n",
       "      <th>Directors</th>\n",
       "      <th>IMDb Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>El circo</td>\n",
       "      <td>1928</td>\n",
       "      <td>Charles Chaplin</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Terminator</td>\n",
       "      <td>1984</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Mandarinas</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zaza Urushadze</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>A Silent Voice</td>\n",
       "      <td>2016</td>\n",
       "      <td>Naoko Yamada</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Winter Sleep</td>\n",
       "      <td>Sueño de invierno</td>\n",
       "      <td>Nuri Bilge Ceylan</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title                 year             Directors  \\\n",
       "0            Cadena perpetua                   1994        Frank Darabont   \n",
       "1                 El padrino                   1972  Francis Ford Coppola   \n",
       "2       El padrino: Parte II                   1974  Francis Ford Coppola   \n",
       "3        El caballero oscuro                   2008     Christopher Nolan   \n",
       "4      12 hombres sin piedad                   1957          Sidney Lumet   \n",
       "..                         ...                  ...                   ...   \n",
       "245                 El circo                   1928       Charles Chaplin   \n",
       "246               Terminator                   1984         James Cameron   \n",
       "247               Mandarinas                   2013        Zaza Urushadze   \n",
       "248           A Silent Voice                   2016          Naoko Yamada   \n",
       "249              Winter Sleep   Sueño de invierno       Nuri Bilge Ceylan   \n",
       "\n",
       "     IMDb Rating  \n",
       "0            9.2  \n",
       "1            9.1  \n",
       "2            9.0  \n",
       "3            9.0  \n",
       "4            8.9  \n",
       "..           ...  \n",
       "245          8.0  \n",
       "246          8.0  \n",
       "247          8.0  \n",
       "248          8.0  \n",
       "249          8.0  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_html(url)\n",
    "movies = data[0]\n",
    "movies[\"Directors\"] = test_list\n",
    "\n",
    "\n",
    "\n",
    "lst = [i.split(\"(\") for i in movies_df[\"Rank & Title\"]]\n",
    "name = [i[0].split(\".\") for i in lst]\n",
    "\n",
    "title= [x[1] for x in name]\n",
    "year = [i[1].replace(\")\",\"\") for i in lst]\n",
    "\n",
    "\n",
    "\n",
    "movies[\"Title\"] = title\n",
    "movies[\"year\"]=year\n",
    "\n",
    "movies_full = movies[[\"Title\",\"year\",\"Directors\",\"IMDb Rating\"]]\n",
    "\n",
    "movies_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'https://www.imdb.com/list/ls009796553/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeej it works\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d5550672d711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import requests\n",
    "link = 'https://www.imdb.com/list/ls009796553/'\n",
    "page = requests.get(link)\n",
    "\n",
    "# error handling\n",
    "if(page.status_code==200):\n",
    "    print('jeej it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "    print('status_code: ', page.status_code)\n",
    "    \n",
    "    \n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "list(soup.children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = [soup.find_all(class_='lister-item-header')[i].get_text().split(\"\\n\") for i in range(0,10)]\n",
    "title\n",
    "\n",
    "only_title = [i[2] for i in title]\n",
    "date = [i[3].replace(\"(\",\"\").replace(\")\",\"\") for i in title]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The monstrous spirit of a slain child murderer seeks revenge by invading the dreams of teenagers whose parents were responsible for his untimely death.',\n",
       " 'The victims of an encephalitis epidemic many years ago have been catatonic ever since, but now a new drug offers the prospect of reviving them.',\n",
       " 'Two sisters join the first female professional baseball league and struggle to help it succeed amidst their own growing rivalry.',\n",
       " 'A father becomes worried when a local gangster befriends his son in the Bronx in the 1960s.',\n",
       " 'When a boy prays for a chance to have a family if the California Angels win the pennant, angels are assigned to make that possible.',\n",
       " 'In Canton, Mississippi, a fearless young lawyer and his assistant defend a black man accused of murdering two white men who raped his ten-year-old daughter, inciting violent retribution and revenge from the Ku Klux Klan.',\n",
       " 'In 1839, the revolt of Mende captives aboard a Spanish owned ship causes a major controversy in the United States when the ship is captured off the coast of Long Island. The courts must decide whether the Mende are slaves or legally free.',\n",
       " 'A \"National Geographic\" film crew is taken hostage by an insane hunter, who forces them along on his quest to capture the world\\'s largest - and deadliest - snake.',\n",
       " 'Russell, single father balances his work as a lawyer with the care of his five-year-old son, after his wife abandoned them. When she reappears creating turmoil, he must deal with his new love interest and a job opportunity of a lifetime.',\n",
       " 'A former neo-nazi skinhead tries to prevent his younger brother from going down the same wrong path that he did.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = soup.find_all(\"p\",class_='')[0]\n",
    "\n",
    "summary = [soup.find_all(\"p\",class_='')[i].get_text().replace(\"\\n    \",\"\") for i in range(0,10)]\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pesadilla en Elm Street</td>\n",
       "      <td>1984</td>\n",
       "      <td>The monstrous spirit of a slain child murderer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despertares</td>\n",
       "      <td>1990</td>\n",
       "      <td>The victims of an encephalitis epidemic many y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ellas dan el golpe</td>\n",
       "      <td>1992</td>\n",
       "      <td>Two sisters join the first female professional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Una historia del Bronx</td>\n",
       "      <td>1993</td>\n",
       "      <td>A father becomes worried when a local gangster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ángeles</td>\n",
       "      <td>1994</td>\n",
       "      <td>When a boy prays for a chance to have a family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiempo de matar</td>\n",
       "      <td>1996</td>\n",
       "      <td>In Canton, Mississippi, a fearless young lawye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amistad</td>\n",
       "      <td>1997</td>\n",
       "      <td>In 1839, the revolt of Mende captives aboard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anaconda</td>\n",
       "      <td>1997</td>\n",
       "      <td>A \"National Geographic\" film crew is taken hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Cool, Dry Place</td>\n",
       "      <td>1998</td>\n",
       "      <td>Russell, single father balances his work as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>American History X</td>\n",
       "      <td>1998</td>\n",
       "      <td>A former neo-nazi skinhead tries to prevent hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Title  Year  \\\n",
       "0  Pesadilla en Elm Street  1984   \n",
       "1              Despertares  1990   \n",
       "2       Ellas dan el golpe  1992   \n",
       "3   Una historia del Bronx  1993   \n",
       "4                  Ángeles  1994   \n",
       "5          Tiempo de matar  1996   \n",
       "6                  Amistad  1997   \n",
       "7                 Anaconda  1997   \n",
       "8        A Cool, Dry Place  1998   \n",
       "9       American History X  1998   \n",
       "\n",
       "                                             Summary  \n",
       "0  The monstrous spirit of a slain child murderer...  \n",
       "1  The victims of an encephalitis epidemic many y...  \n",
       "2  Two sisters join the first female professional...  \n",
       "3  A father becomes worried when a local gangster...  \n",
       "4  When a boy prays for a chance to have a family...  \n",
       "5  In Canton, Mississippi, a fearless young lawye...  \n",
       "6  In 1839, the revolt of Mende captives aboard a...  \n",
       "7  A \"National Geographic\" film crew is taken hos...  \n",
       "8  Russell, single father balances his work as a ...  \n",
       "9  A former neo-nazi skinhead tries to prevent hi...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Title\"] = only_title\n",
    "df[\"Year\"] = date\n",
    "df[\"Summary\"] = summary\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city: Vancouver\n",
      "http://api.openweathermap.org/data/2.5/weather?q=Vancouver&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Temperature: 5.42 Wind speed: 1 Description: \"light rain\"'"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'\n",
    "\n",
    "\n",
    "\n",
    "def weather(city):\n",
    "    test = print(url)\n",
    "    link = test\n",
    "    page = requests.get(url)\n",
    "    page.content\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    converting = list(soup)\n",
    "    converting2= str(page.content).split(\",\")\n",
    "    description = converting2[4].split(\":\")[1].replace(\"'\",\"\")\n",
    "    wind_Speed = converting2[14].split(\":\")[2]\n",
    "    temperature = converting2[7].split(\":\")[2]\n",
    "    return \"Temperature: \" +temperature + \" Wind speed: \" + wind_Speed + \" Description: \" + description\n",
    "\n",
    "weather(city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3c0495e2ef84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "url = 'http://books.toscrape.com/'\n",
    "page = requests.get(url)\n",
    "\n",
    "    \n",
    "    \n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "test = list(soup.children)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = [soup.find_all('h3')[i].get_text() for i in range(0,20)]\n",
    "\n",
    "price = [soup.find_all('p',{\"class\":\"price_color\"})[i].get_text().split(\"\\n\") for i in range(0,20)]\n",
    "stock = [soup.find_all('p',{\"class\":\"instock availability\"})[i].get_text().replace(\"\\n\\n    \\n        \",\"\").replace(\"\\n    \\n\",\"\") for i in range(0,20)]\n",
    "\n",
    "price2 = [y for x in price for y in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title   Price     Stock\n",
       "0                      A Light in the ...  £51.77  In stock\n",
       "1                      Tipping the Velvet  £53.74  In stock\n",
       "2                              Soumission  £50.10  In stock\n",
       "3                           Sharp Objects  £47.82  In stock\n",
       "4            Sapiens: A Brief History ...  £54.23  In stock\n",
       "5                         The Requiem Red  £22.65  In stock\n",
       "6            The Dirty Little Secrets ...  £33.34  In stock\n",
       "7                 The Coming Woman: A ...  £17.93  In stock\n",
       "8                     The Boys in the ...  £22.60  In stock\n",
       "9                         The Black Maria  £52.15  In stock\n",
       "10  Starving Hearts (Triangular Trade ...  £13.99  In stock\n",
       "11                  Shakespeare's Sonnets  £20.66  In stock\n",
       "12                            Set Me Free  £17.46  In stock\n",
       "13    Scott Pilgrim's Precious Little ...  £52.29  In stock\n",
       "14                      Rip it Up and ...  £35.02  In stock\n",
       "15                  Our Band Could Be ...  £57.25  In stock\n",
       "16                                   Olio  £23.88  In stock\n",
       "17        Mesaerion: The Best Science ...  £37.59  In stock\n",
       "18           Libertarianism for Beginners  £51.33  In stock\n",
       "19                It's Only the Himalayas  £45.17  In stock"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Title\"]= title\n",
    "df[\"Price\"]=price2\n",
    "df[\"Stock\"]=stock\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 100 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe.\n",
    "***Hint:*** Here the displayed number of earthquakes per page is 20, but you can easily move to the next page by looping through the desired number of pages and adding it to the end of the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.emsc-csem.org/Earthquake/?view=1',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=2',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=3',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=4',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=5']"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/?view='\n",
    "\n",
    "# This is how you will loop through each page:\n",
    "number_of_pages = int(100/20)\n",
    "each_page_urls = []\n",
    "\n",
    "for n in range(1, number_of_pages+1):\n",
    "    link = url+str(n)\n",
    "    each_page_urls.append(link)\n",
    "    \n",
    "each_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have been able to extract info from one url. Could I get the full code? I think it's very interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "url1 = 'https://www.emsc-csem.org/Earthquake/?view=1'\n",
    "# your code here\n",
    "\n",
    "url2 = 'https://www.emsc-csem.org/Earthquake/?view=2'\n",
    "url3 = 'https://www.emsc-csem.org/Earthquake/?view=3'\n",
    "url4 = 'https://www.emsc-csem.org/Earthquake/?view=4'\n",
    "url5 = 'https://www.emsc-csem.org/Earthquake/?view=5'\n",
    "page = requests.get(url)\n",
    " \n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "test = list(soup.children)\n",
    "\n",
    "\n",
    "data1 = pd.read_html(url1)\n",
    "data2 = pd.read_html(url2)\n",
    "data3 = pd.read_html(url3)\n",
    "data4 = pd.read_html(url4)\n",
    "data5 = pd.read_html(url5)\n",
    "\n",
    "\n",
    "data_earthquaks = data1[3]\n",
    "\n",
    "earthquakes = pd.DataFrame()\n",
    "\n",
    "earthquakes[\"Dates & time\"] = data_earthquaks[\"Date & Time UTC\"][\"12345678910»\"]\n",
    "earthquakes[\"Latitude\"] = data_earthquaks[\"Latitude degrees\"][\"12345678910»\"]\n",
    "earthquakes[\"Latitude2\"] = data_earthquaks[\"Latitude degrees\"][\"12345678910».1\"]\n",
    "earthquakes[\"Longitud\"] = data_earthquaks[\"Longitude degrees\"][\"12345678910»\"]\n",
    "earthquakes[\"Longitud2\"] = data_earthquaks[\"Longitude degrees\"][\"12345678910».1\"]\n",
    "earthquakes[\"Region Name\"] = data_earthquaks[\"Last update [-]\"][\"12345678910»\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates &amp; time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Latitude2</th>\n",
       "      <th>Longitud</th>\n",
       "      <th>Longitud2</th>\n",
       "      <th>Region Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-23 18:26:39.309min ago</td>\n",
       "      <td>42.93</td>\n",
       "      <td>N</td>\n",
       "      <td>0.21</td>\n",
       "      <td>E</td>\n",
       "      <td>PYRENEES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-23 18:14:37.021min ago</td>\n",
       "      <td>27.32</td>\n",
       "      <td>S</td>\n",
       "      <td>70.66</td>\n",
       "      <td>W</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-23 18:05:41.030min ago</td>\n",
       "      <td>6.65</td>\n",
       "      <td>N</td>\n",
       "      <td>126.60</td>\n",
       "      <td>E</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-23 17:51:20.245min ago</td>\n",
       "      <td>37.13</td>\n",
       "      <td>N</td>\n",
       "      <td>117.55</td>\n",
       "      <td>W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-23 17:36:57.059min ago</td>\n",
       "      <td>0.78</td>\n",
       "      <td>S</td>\n",
       "      <td>127.64</td>\n",
       "      <td>E</td>\n",
       "      <td>HALMAHERA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-23 17:20:17.81hr 16min ago</td>\n",
       "      <td>19.23</td>\n",
       "      <td>N</td>\n",
       "      <td>155.37</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-23 17:08:28.01hr 27min ago</td>\n",
       "      <td>9.08</td>\n",
       "      <td>S</td>\n",
       "      <td>114.47</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTH OF BALI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-23 17:06:47.01hr 29min ago</td>\n",
       "      <td>8.52</td>\n",
       "      <td>S</td>\n",
       "      <td>110.54</td>\n",
       "      <td>E</td>\n",
       "      <td>JAVA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-23 16:58:04.51hr 38min ago</td>\n",
       "      <td>37.35</td>\n",
       "      <td>N</td>\n",
       "      <td>2.08</td>\n",
       "      <td>W</td>\n",
       "      <td>SPAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-23 16:56:15.71hr 40min ago</td>\n",
       "      <td>31.72</td>\n",
       "      <td>S</td>\n",
       "      <td>179.75</td>\n",
       "      <td>E</td>\n",
       "      <td>KERMADEC ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-11-23 16:41:35.41hr 54min ago</td>\n",
       "      <td>37.88</td>\n",
       "      <td>N</td>\n",
       "      <td>26.72</td>\n",
       "      <td>E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-11-23 16:13:06.12hr 23min ago</td>\n",
       "      <td>18.75</td>\n",
       "      <td>N</td>\n",
       "      <td>65.10</td>\n",
       "      <td>W</td>\n",
       "      <td>VIRGIN ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-11-23 16:05:30.22hr 30min ago</td>\n",
       "      <td>28.12</td>\n",
       "      <td>N</td>\n",
       "      <td>16.70</td>\n",
       "      <td>W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-11-23 16:03:30.02hr 32min ago</td>\n",
       "      <td>9.80</td>\n",
       "      <td>N</td>\n",
       "      <td>83.83</td>\n",
       "      <td>W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-11-23 16:02:13.02hr 34min ago</td>\n",
       "      <td>38.52</td>\n",
       "      <td>N</td>\n",
       "      <td>29.85</td>\n",
       "      <td>W</td>\n",
       "      <td>AZORES ISLANDS, PORTUGAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-11-23 16:02:05.52hr 34min ago</td>\n",
       "      <td>39.44</td>\n",
       "      <td>N</td>\n",
       "      <td>38.55</td>\n",
       "      <td>E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-11-23 15:40:23.72hr 56min ago</td>\n",
       "      <td>56.12</td>\n",
       "      <td>N</td>\n",
       "      <td>164.51</td>\n",
       "      <td>E</td>\n",
       "      <td>KOMANDORSKIYE OSTROVA REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-11-23 15:35:45.03hr 00min ago</td>\n",
       "      <td>38.62</td>\n",
       "      <td>N</td>\n",
       "      <td>29.82</td>\n",
       "      <td>W</td>\n",
       "      <td>AZORES ISLANDS, PORTUGAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-11-23 15:31:54.73hr 04min ago</td>\n",
       "      <td>37.93</td>\n",
       "      <td>N</td>\n",
       "      <td>27.09</td>\n",
       "      <td>E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-11-23 15:28:28.03hr 07min ago</td>\n",
       "      <td>18.88</td>\n",
       "      <td>N</td>\n",
       "      <td>71.01</td>\n",
       "      <td>W</td>\n",
       "      <td>DOMINICAN REPUBLIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-11-23 15:25:26.63hr 10min ago</td>\n",
       "      <td>36.95</td>\n",
       "      <td>N</td>\n",
       "      <td>27.40</td>\n",
       "      <td>E</td>\n",
       "      <td>DODECANESE IS.-TURKEY BORDER REG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-11-23 15:25:22.23hr 11min ago</td>\n",
       "      <td>51.71</td>\n",
       "      <td>N</td>\n",
       "      <td>179.62</td>\n",
       "      <td>E</td>\n",
       "      <td>RAT ISLANDS, ALEUTIAN ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-11-23 15:19:32.93hr 16min ago</td>\n",
       "      <td>36.48</td>\n",
       "      <td>N</td>\n",
       "      <td>7.61</td>\n",
       "      <td>W</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-11-23 15:13:19.03hr 23min ago</td>\n",
       "      <td>17.95</td>\n",
       "      <td>N</td>\n",
       "      <td>66.93</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-11-23 15:11:30.03hr 24min ago</td>\n",
       "      <td>36.70</td>\n",
       "      <td>N</td>\n",
       "      <td>141.00</td>\n",
       "      <td>E</td>\n",
       "      <td>NEAR EAST COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-11-23 14:51:08.03hr 45min ago</td>\n",
       "      <td>8.10</td>\n",
       "      <td>S</td>\n",
       "      <td>124.03</td>\n",
       "      <td>E</td>\n",
       "      <td>KEPULAUAN ALOR, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-11-23 14:48:30.03hr 47min ago</td>\n",
       "      <td>10.56</td>\n",
       "      <td>N</td>\n",
       "      <td>85.33</td>\n",
       "      <td>W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-11-23 14:48:12.43hr 48min ago</td>\n",
       "      <td>34.30</td>\n",
       "      <td>N</td>\n",
       "      <td>25.13</td>\n",
       "      <td>E</td>\n",
       "      <td>CRETE, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-11-23 14:42:32.03hr 53min ago</td>\n",
       "      <td>1.16</td>\n",
       "      <td>N</td>\n",
       "      <td>127.43</td>\n",
       "      <td>E</td>\n",
       "      <td>HALMAHERA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-11-23 14:26:24.94hr 09min ago</td>\n",
       "      <td>31.91</td>\n",
       "      <td>N</td>\n",
       "      <td>48.98</td>\n",
       "      <td>E</td>\n",
       "      <td>WESTERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-11-23 14:20:40.04hr 15min ago</td>\n",
       "      <td>28.51</td>\n",
       "      <td>S</td>\n",
       "      <td>70.29</td>\n",
       "      <td>W</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020-11-23 14:19:09.14hr 17min ago</td>\n",
       "      <td>37.90</td>\n",
       "      <td>N</td>\n",
       "      <td>26.65</td>\n",
       "      <td>E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020-11-23 14:12:42.04hr 23min ago</td>\n",
       "      <td>8.85</td>\n",
       "      <td>S</td>\n",
       "      <td>110.34</td>\n",
       "      <td>E</td>\n",
       "      <td>JAVA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-11-23 14:10:06.24hr 26min ago</td>\n",
       "      <td>36.37</td>\n",
       "      <td>N</td>\n",
       "      <td>97.36</td>\n",
       "      <td>W</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-11-23 14:08:51.34hr 27min ago</td>\n",
       "      <td>38.17</td>\n",
       "      <td>N</td>\n",
       "      <td>117.84</td>\n",
       "      <td>W</td>\n",
       "      <td>NEVADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020-11-23 13:49:18.84hr 47min ago</td>\n",
       "      <td>36.91</td>\n",
       "      <td>N</td>\n",
       "      <td>104.90</td>\n",
       "      <td>W</td>\n",
       "      <td>NEW MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2020-11-23 13:41:45.34hr 54min ago</td>\n",
       "      <td>28.59</td>\n",
       "      <td>N</td>\n",
       "      <td>87.20</td>\n",
       "      <td>E</td>\n",
       "      <td>WESTERN XIZANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-11-23 13:32:02.05hr 04min ago</td>\n",
       "      <td>28.03</td>\n",
       "      <td>S</td>\n",
       "      <td>70.81</td>\n",
       "      <td>W</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-11-23 13:30:20.25hr 06min ago</td>\n",
       "      <td>38.75</td>\n",
       "      <td>N</td>\n",
       "      <td>122.71</td>\n",
       "      <td>W</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020-11-23 13:26:33.05hr 09min ago</td>\n",
       "      <td>5.66</td>\n",
       "      <td>S</td>\n",
       "      <td>129.14</td>\n",
       "      <td>E</td>\n",
       "      <td>BANDA SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-11-23 13:06:20.45hr 30min ago</td>\n",
       "      <td>38.19</td>\n",
       "      <td>N</td>\n",
       "      <td>117.74</td>\n",
       "      <td>W</td>\n",
       "      <td>NEVADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-11-23 13:03:38.05hr 32min ago</td>\n",
       "      <td>24.51</td>\n",
       "      <td>S</td>\n",
       "      <td>68.81</td>\n",
       "      <td>W</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-11-23 13:02:16.05hr 34min ago</td>\n",
       "      <td>21.23</td>\n",
       "      <td>S</td>\n",
       "      <td>68.72</td>\n",
       "      <td>W</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2020-11-23 12:56:50.05hr 39min ago</td>\n",
       "      <td>20.19</td>\n",
       "      <td>N</td>\n",
       "      <td>70.15</td>\n",
       "      <td>W</td>\n",
       "      <td>DOMINICAN REPUBLIC REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2020-11-23 12:55:38.25hr 40min ago</td>\n",
       "      <td>46.74</td>\n",
       "      <td>N</td>\n",
       "      <td>112.54</td>\n",
       "      <td>W</td>\n",
       "      <td>WESTERN MONTANA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2020-11-23 11:43:28.76hr 52min ago</td>\n",
       "      <td>35.88</td>\n",
       "      <td>N</td>\n",
       "      <td>117.71</td>\n",
       "      <td>W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2020-11-23 11:34:30.07hr 01min ago</td>\n",
       "      <td>8.53</td>\n",
       "      <td>N</td>\n",
       "      <td>82.87</td>\n",
       "      <td>W</td>\n",
       "      <td>PANAMA-COSTA RICA BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2020-11-23 11:31:51.27hr 04min ago</td>\n",
       "      <td>34.95</td>\n",
       "      <td>N</td>\n",
       "      <td>25.39</td>\n",
       "      <td>E</td>\n",
       "      <td>CRETE, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2020-11-23 11:23:30.07hr 12min ago</td>\n",
       "      <td>8.41</td>\n",
       "      <td>N</td>\n",
       "      <td>83.05</td>\n",
       "      <td>W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-11-23 11:18:03.07hr 18min ago</td>\n",
       "      <td>56.71</td>\n",
       "      <td>S</td>\n",
       "      <td>67.89</td>\n",
       "      <td>W</td>\n",
       "      <td>DRAKE PASSAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "      <td>12345678910»</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Dates & time       Latitude      Latitude2  \\\n",
       "0       2020-11-23 18:26:39.309min ago          42.93              N   \n",
       "1       2020-11-23 18:14:37.021min ago          27.32              S   \n",
       "2       2020-11-23 18:05:41.030min ago           6.65              N   \n",
       "3       2020-11-23 17:51:20.245min ago          37.13              N   \n",
       "4       2020-11-23 17:36:57.059min ago           0.78              S   \n",
       "5   2020-11-23 17:20:17.81hr 16min ago          19.23              N   \n",
       "6   2020-11-23 17:08:28.01hr 27min ago           9.08              S   \n",
       "7   2020-11-23 17:06:47.01hr 29min ago           8.52              S   \n",
       "8   2020-11-23 16:58:04.51hr 38min ago          37.35              N   \n",
       "9   2020-11-23 16:56:15.71hr 40min ago          31.72              S   \n",
       "10  2020-11-23 16:41:35.41hr 54min ago          37.88              N   \n",
       "11  2020-11-23 16:13:06.12hr 23min ago          18.75              N   \n",
       "12  2020-11-23 16:05:30.22hr 30min ago          28.12              N   \n",
       "13  2020-11-23 16:03:30.02hr 32min ago           9.80              N   \n",
       "14  2020-11-23 16:02:13.02hr 34min ago          38.52              N   \n",
       "15  2020-11-23 16:02:05.52hr 34min ago          39.44              N   \n",
       "16  2020-11-23 15:40:23.72hr 56min ago          56.12              N   \n",
       "17  2020-11-23 15:35:45.03hr 00min ago          38.62              N   \n",
       "18  2020-11-23 15:31:54.73hr 04min ago          37.93              N   \n",
       "19  2020-11-23 15:28:28.03hr 07min ago          18.88              N   \n",
       "20  2020-11-23 15:25:26.63hr 10min ago          36.95              N   \n",
       "21  2020-11-23 15:25:22.23hr 11min ago          51.71              N   \n",
       "22  2020-11-23 15:19:32.93hr 16min ago          36.48              N   \n",
       "23  2020-11-23 15:13:19.03hr 23min ago          17.95              N   \n",
       "24  2020-11-23 15:11:30.03hr 24min ago          36.70              N   \n",
       "25  2020-11-23 14:51:08.03hr 45min ago           8.10              S   \n",
       "26  2020-11-23 14:48:30.03hr 47min ago          10.56              N   \n",
       "27  2020-11-23 14:48:12.43hr 48min ago          34.30              N   \n",
       "28  2020-11-23 14:42:32.03hr 53min ago           1.16              N   \n",
       "29  2020-11-23 14:26:24.94hr 09min ago          31.91              N   \n",
       "30  2020-11-23 14:20:40.04hr 15min ago          28.51              S   \n",
       "31  2020-11-23 14:19:09.14hr 17min ago          37.90              N   \n",
       "32  2020-11-23 14:12:42.04hr 23min ago           8.85              S   \n",
       "33  2020-11-23 14:10:06.24hr 26min ago          36.37              N   \n",
       "34  2020-11-23 14:08:51.34hr 27min ago          38.17              N   \n",
       "35  2020-11-23 13:49:18.84hr 47min ago          36.91              N   \n",
       "36  2020-11-23 13:41:45.34hr 54min ago          28.59              N   \n",
       "37  2020-11-23 13:32:02.05hr 04min ago          28.03              S   \n",
       "38  2020-11-23 13:30:20.25hr 06min ago          38.75              N   \n",
       "39  2020-11-23 13:26:33.05hr 09min ago           5.66              S   \n",
       "40  2020-11-23 13:06:20.45hr 30min ago          38.19              N   \n",
       "41  2020-11-23 13:03:38.05hr 32min ago          24.51              S   \n",
       "42  2020-11-23 13:02:16.05hr 34min ago          21.23              S   \n",
       "43  2020-11-23 12:56:50.05hr 39min ago          20.19              N   \n",
       "44  2020-11-23 12:55:38.25hr 40min ago          46.74              N   \n",
       "45  2020-11-23 11:43:28.76hr 52min ago          35.88              N   \n",
       "46  2020-11-23 11:34:30.07hr 01min ago           8.53              N   \n",
       "47  2020-11-23 11:31:51.27hr 04min ago          34.95              N   \n",
       "48  2020-11-23 11:23:30.07hr 12min ago           8.41              N   \n",
       "49  2020-11-23 11:18:03.07hr 18min ago          56.71              S   \n",
       "50                                 NaN            NaN            NaN   \n",
       "51                       12345678910»  12345678910»  12345678910»   \n",
       "52                                 NaN            NaN            NaN   \n",
       "\n",
       "         Longitud      Longitud2                       Region Name  \n",
       "0            0.21              E                          PYRENEES  \n",
       "1           70.66              W                    ATACAMA, CHILE  \n",
       "2          126.60              E             MINDANAO, PHILIPPINES  \n",
       "3          117.55              W                CENTRAL CALIFORNIA  \n",
       "4          127.64              E              HALMAHERA, INDONESIA  \n",
       "5          155.37              W          ISLAND OF HAWAII, HAWAII  \n",
       "6          114.47              E          SOUTH OF BALI, INDONESIA  \n",
       "7          110.54              E                   JAVA, INDONESIA  \n",
       "8            2.08              W                             SPAIN  \n",
       "9          179.75              E           KERMADEC ISLANDS REGION  \n",
       "10          26.72              E        DODECANESE ISLANDS, GREECE  \n",
       "11          65.10              W             VIRGIN ISLANDS REGION  \n",
       "12          16.70              W      CANARY ISLANDS, SPAIN REGION  \n",
       "13          83.83              W                        COSTA RICA  \n",
       "14          29.85              W          AZORES ISLANDS, PORTUGAL  \n",
       "15          38.55              E                    EASTERN TURKEY  \n",
       "16         164.51              E      KOMANDORSKIYE OSTROVA REGION  \n",
       "17          29.82              W          AZORES ISLANDS, PORTUGAL  \n",
       "18          27.09              E                    WESTERN TURKEY  \n",
       "19          71.01              W                DOMINICAN REPUBLIC  \n",
       "20          27.40              E  DODECANESE IS.-TURKEY BORDER REG  \n",
       "21         179.62              E     RAT ISLANDS, ALEUTIAN ISLANDS  \n",
       "22           7.61              W               STRAIT OF GIBRALTAR  \n",
       "23          66.93              W                       PUERTO RICO  \n",
       "24         141.00              E  NEAR EAST COAST OF HONSHU, JAPAN  \n",
       "25         124.03              E         KEPULAUAN ALOR, INDONESIA  \n",
       "26          85.33              W                        COSTA RICA  \n",
       "27          25.13              E                     CRETE, GREECE  \n",
       "28         127.43              E              HALMAHERA, INDONESIA  \n",
       "29          48.98              E                      WESTERN IRAN  \n",
       "30          70.29              W                    ATACAMA, CHILE  \n",
       "31          26.65              E        DODECANESE ISLANDS, GREECE  \n",
       "32         110.34              E                   JAVA, INDONESIA  \n",
       "33          97.36              W                          OKLAHOMA  \n",
       "34         117.84              W                            NEVADA  \n",
       "35         104.90              W                        NEW MEXICO  \n",
       "36          87.20              E                    WESTERN XIZANG  \n",
       "37          70.81              W                    ATACAMA, CHILE  \n",
       "38         122.71              W               NORTHERN CALIFORNIA  \n",
       "39         129.14              E                         BANDA SEA  \n",
       "40         117.74              W                            NEVADA  \n",
       "41          68.81              W                ANTOFAGASTA, CHILE  \n",
       "42          68.72              W                ANTOFAGASTA, CHILE  \n",
       "43          70.15              W         DOMINICAN REPUBLIC REGION  \n",
       "44         112.54              W                   WESTERN MONTANA  \n",
       "45         117.71              W                CENTRAL CALIFORNIA  \n",
       "46          82.87              W   PANAMA-COSTA RICA BORDER REGION  \n",
       "47          25.39              E                     CRETE, GREECE  \n",
       "48          83.05              W                        COSTA RICA  \n",
       "49          67.89              W                     DRAKE PASSAGE  \n",
       "50            NaN            NaN                               NaN  \n",
       "51  12345678910»  12345678910»                     12345678910»  \n",
       "52            NaN            NaN                               NaN  "
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
